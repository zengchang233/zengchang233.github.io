@inproceedings{10.1145/3552466.3556527,
author = {Liu, Xiaohui and Liu, Meng and Zhang, Lin and Zhang, Linjuan and Zeng, Chang and Li, Kai and Li, Nan and Lee, Kong Aik and Wang, Longbiao and Dang, Jianwu},
title = {Deep Spectro-Temporal Artifacts for Detecting Synthesized Speech},
year = {2022},
isbn = {9781450394963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552466.3556527},
doi = {10.1145/3552466.3556527},
abstract = {The Audio Deep Synthesis Detection (ADD) Challenge has been held to detect generated human-like speech. With our submitted system, this paper provides an overall assessment of track 1 (Low-quality Fake Audio Detection) and track 2 (Partially Fake Audio Detection). In this paper, spectro-temporal artifacts were detected using raw temporal signals, spectral features, as well as deep embedding features. To address track 1, low-quality data augmentation, domain adaptation via finetuning, and various complementary feature information fusion were aggregated in our system. Furthermore, we analyzed the clustering characteristics of subsystems with different features by visualization method and explained the effectiveness of our proposed greedy fusion strategy. As for track 2, frame transition and smoothing were detected using self-supervised learning structure to capture the manipulation of PF attacks in the time domain. We ranked 4th and 5th in track 1 and track 2, respectively.},
booktitle = {Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia},
pages = {69â€“75},
numpages = {7},
keywords = {spectro-temporal, audio deep synthesis detection, self-supervised learning, frame transition, domain adaptation, greedy fusion},
location = {Lisboa, Portugal},
series = {DDAM '22}
}