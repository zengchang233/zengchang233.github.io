<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: May 25, 2023 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.e5d7adca760216d3b7e28ea434e81f6f.css" />

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  


























  
  
  






  <meta name="author" content="Chang ZENG 曾畅　曾 暢 (ソウ チョウ)" />





  

<meta name="description" content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder." />



<link rel="alternate" hreflang="en-us" href="https://zengchang233.github.io/tag/speaker-recognition-/-antispoofing/" />
<link rel="canonical" href="https://zengchang233.github.io/tag/speaker-recognition-/-antispoofing/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hudc967fd05c8db002c2406ec36adacb20_394509_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hudc967fd05c8db002c2406ec36adacb20_394509_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@wowchemy" />
  <meta property="twitter:creator" content="@wowchemy" />
<meta property="twitter:image" content="https://zengchang233.github.io/media/icon_hudc967fd05c8db002c2406ec36adacb20_394509_512x512_fill_lanczos_center_3.png" />
<meta property="og:site_name" content="Chang ZENG" />
<meta property="og:url" content="https://zengchang233.github.io/tag/speaker-recognition-/-antispoofing/" />
<meta property="og:title" content="Speaker Recognition / Antispoofing | Chang ZENG" />
<meta property="og:description" content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder." /><meta property="og:image" content="https://zengchang233.github.io/media/icon_hudc967fd05c8db002c2406ec36adacb20_394509_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  










  
  
  

  
  
    <link rel="alternate" href="/tag/speaker-recognition-/-antispoofing/index.xml" type="application/rss+xml" title="Chang ZENG" />
  

  


  
  <title>Speaker Recognition / Antispoofing | Chang ZENG</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   "  >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
    












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Chang ZENG</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Chang ZENG</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Featured Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#skills"><span>Skills</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#experience"><span>Experience</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
            
            <li class="nav-item d-none d-lg-inline-flex">
              <a class="nav-link" href="https://twitter.com/zzzzzc12" data-toggle="tooltip" data-placement="bottom" title="Follow me on Twitter" target="_blank" rel="noopener" aria-label="Follow me on Twitter">
                <i class="fab fa-twitter" aria-hidden="true"></i>
              </a>
            </li>
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    













  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Speaker Recognition / Antispoofing</h1>

  

  
</div>



<div class="universal-wrapper">
  


  

  
  
    







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/interspeech2023_generalization/" >Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms</a>
    </div>

    
    <a href="/publication/interspeech2023_generalization/"  class="summary-link">
      <div class="article-style">
        The ability of countermeasure models to generalize from seen speech synthesis methods to unseen ones has been investigated in the ASVspoof challenge. However, a new mismatch scenario in which fake audio may be generated from real audio with unseen genres has not been studied thoroughly. To this end, we first use five different vocoders to create a new dataset called CN-Spoof based on the CN-Celeb1&amp;2 datasets. Then, we design two auxiliary objectives for regularization via meta-optimization and a genre alignment module, respectively, and combine them with the main anti-spoofing objective using learnable weights for multiple loss terms.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Chang Zeng</span>, <span >
      Xin Wang</span>, <span >
      Xiaoxiao Miao</span>, <span >
      Erica Cooper</span>, <span >
      Junichi Yamagishi</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2305.10940.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/interspeech2023_generalization/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/nii-yamagishilab/Generalization_of_CMs_regularizations" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/speaker_verification/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2305.10940" target="_blank" rel="noopener">
    ArXiv Link</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      <a href="/publication/interspeech2023_generalization/" >
        <img src="/publication/interspeech2023_generalization/featured_hu86f32c074a357154a8319afdb005e42f_627762_150x0_resize_q75_h2_lanczos_3.webp" height="126" width="150"
            alt="Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms" loading="lazy">
      </a>
    
  </div>
</div>

  
    







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/icassp2023_av_colearning/" >Cross-Modal Audio-Visual Co-Learning for Text-Independent Speaker Verification</a>
    </div>

    
    <a href="/publication/icassp2023_av_colearning/"  class="summary-link">
      <div class="article-style">
        Visual speech (i.e., lip motion) is highly related to auditory speech due to the co-occurrence and synchronization in speech production. This paper investigates this correlation and proposes a cross-modal speech co-learning paradigm. The primary motivation of our cross-modal co-learning method is modeling one modality aided by exploiting knowledge from another modality. Specifically, two cross-modal boosters are introduced based on an audio-visual pseudo-siamese structure to learn the modality-transformed correlation. Inside each booster, a max-feature-map embedded Transformer variant is proposed for modality alignment and enhanced feature generation. The network is co-learned both from scratch and with pretrained models. Experimental results on the test scenarios demonstrate that our proposed method achieves around 60% and 20% average relative performance improvement over baseline unimodal and fusion systems, respectively.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Meng Liu</span>, <span >
      Kong Aik Lee</span>, <span >
      Longbiao Wang</span>, <span >
      Hanyi Zhang</span>, <span >
      Chang Zeng</span>, <span >
      Jianwu Dang</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2302.11254.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/icassp2023_av_colearning/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/speaker_verification/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICASSP49357.2023.10095883" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/10095883" target="_blank" rel="noopener">
    ICASSP Link</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      <a href="/publication/icassp2023_av_colearning/" >
        <img src="/publication/icassp2023_av_colearning/featured_hu3a0063144fa60b702a61cc3db4cff908_449769_150x0_resize_q75_h2_lanczos.webp" height="72" width="150"
            alt="Cross-Modal Audio-Visual Co-Learning for Text-Independent Speaker Verification" loading="lazy">
      </a>
    
  </div>
</div>

  
    







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/acmmm2022_ddam_workshop/" >Deep Spectro-temporal Artifacts for Detecting Synthesized Speech</a>
    </div>

    
    <a href="/publication/acmmm2022_ddam_workshop/"  class="summary-link">
      <div class="article-style">
        Audio Deep Synthesis Detection, Spectro-temporal, Domain Adaptation, Self-Supervised Learning, Frame transition, Greedy Fusion.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Xiaohui Liu</span>, <span >
      Meng Liu</span>, <span >
      Lin Zhang</span>, <span >
      Linjuan Zhang</span>, <span >
      Chang Zeng</span>, <span >
      Kai Li</span>, <span >
      Nan Li</span>, <span >
      Kong Aik Lee</span>, <span >
      Longbiao Wang</span>, <span >
      Jianwu Dang</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2210.05254.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/acmmm2022_ddam_workshop/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/speaker_verification/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3552466.3556527" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://dl.acm.org/doi/abs/10.1145/3552466.3556527" target="_blank" rel="noopener">
    ACMMM Link</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      <a href="/publication/acmmm2022_ddam_workshop/" >
        <img src="/publication/acmmm2022_ddam_workshop/featured_hu3cc7554004a4f82bf46186159a47e918_386582_150x0_resize_q75_h2_lanczos.webp" height="140" width="150"
            alt="Deep Spectro-temporal Artifacts for Detecting Synthesized Speech" loading="lazy">
      </a>
    
  </div>
</div>

  
    







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/interspeech2022_antispoofing/" >Data Augmentation Using McAdams-Coefficient-Based Speaker Anonymization for Fake Audio Detection</a>
    </div>

    
    <a href="/publication/interspeech2022_antispoofing/"  class="summary-link">
      <div class="article-style">
        fake audio detection, data augmentation, McAdams coefficients, speaker anonymization.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Kai Li</span>, <span >
      Sheng Li</span>, <span >
      Xugang Lu</span>, <span >
      Masato Akagi</span>, <span >
      Meng Liu</span>, <span >
      Lin Zhang</span>, <span >
      Chang Zeng</span>, <span >
      Longbiao Wang</span>, <span >
      Jianwu Dang</span>, <span >
      Masashi Unoki</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/li22o_interspeech.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/interspeech2022_antispoofing/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/speaker_verification/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.21437/Interspeech.2022-10088" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.isca-speech.org/archive/interspeech_2022/li22o_interspeech.html" target="_blank" rel="noopener">
    INTERSPEECH Link</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      <a href="/publication/interspeech2022_antispoofing/" >
        <img src="/publication/interspeech2022_antispoofing/featured_hu3cc7554004a4f82bf46186159a47e918_155988_150x0_resize_q75_h2_lanczos_3.webp" height="58" width="150"
            alt="Data Augmentation Using McAdams-Coefficient-Based Speaker Anonymization for Fake Audio Detection" loading="lazy">
      </a>
    
  </div>
</div>

  
    







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/interspeech2022/" >Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022</a>
    </div>

    
    <a href="/publication/interspeech2022/"  class="summary-link">
      <div class="article-style">
        Current state-of-the-art automatic speaker verification (ASV) systems are vulnerable to presentation attacks, and several countermeasures (CMs), which distinguish bona fide trials from spoofing ones, have been explored to protect ASV. However, ASV systems and CMs are generally developed and optimized independently without considering their inter-relationship. In this paper, we propose a new spoofing-aware ASV back-end module that efficiently computes a combined ASV score based on speaker similarity and CM score.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Chang Zeng</span>, <span >
      Lin Zhang</span>, <span >
      Meng Liu</span>, <span >
      Junichi Yamagishi</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2209.00423.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/interspeech2022/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/speaker_verification/">
    Project
  </a>
  







  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=gXxP1nn5X6E" target="_blank" rel="noopener">
  Video
</a>



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.21437/Interspeech.2022-10495" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.isca-speech.org/archive/interspeech_2022/zeng22_interspeech.html" target="_blank" rel="noopener">
    INTERSPEECH Link</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      <a href="/publication/interspeech2022/" >
        <img src="/publication/interspeech2022/featured_hu86f32c074a357154a8319afdb005e42f_627762_150x0_resize_q75_h2_lanczos_3.webp" height="126" width="150"
            alt="Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022" loading="lazy">
      </a>
    
  </div>
</div>

  
    







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/taslp_jointasv/" >Joint Speaker Encoder and Neural Back-end Model for Fully End-to-End Automatic Speaker Verification with Multiple Enrollment Utterances</a>
    </div>

    
    <a href="/publication/taslp_jointasv/"  class="summary-link">
      <div class="article-style">
        neural network, automatic speaker verification, deep learning, attention, data augmentation.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Chang Zeng</span>, <span >
      Xiaoxiao Miao</span>, <span >
      Xin Wang</span>, <span >
      Erica Cooper</span>, <span >
      Junichi Yamagishi</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2209.00485.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/taslp_jointasv/cite.bib">
  Cite
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://openslr.org/82/" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/speaker_verification/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.48550/arXiv.2209.00485" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2209.00485" target="_blank" rel="noopener">
    ArXiv Link</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      <a href="/publication/taslp_jointasv/" >
        <img src="/publication/taslp_jointasv/featured_hu8f784d7e0de848d41fd883d1ad23663e_614616_150x0_resize_q75_h2_lanczos.webp" height="160" width="150"
            alt="Joint Speaker Encoder and Neural Back-end Model for Fully End-to-End Automatic Speaker Verification with Multiple Enrollment Utterances" loading="lazy">
      </a>
    
  </div>
</div>

  
    







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/icassp2022/" >Attention Back-end for Automatic Speaker Verification with Multiple Enrollment Utterances</a>
    </div>

    
    <a href="/publication/icassp2022/"  class="summary-link">
      <div class="article-style">
        Probabilistic linear discriminant analysis (PLDA) or cosine similarity has been widely used in traditional speaker verification systems as a back-end technique to measure pairwise similarities. To make better use of multiple enrollment utterances, we propose a novel attention back-end model that is applied to the utterance-level features. Specifically, we use scaled-dot self-attention and feed-forward self-attention networks as architectures that learn the intra-relationships of enrollment utterances.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Chang Zeng</span>, <span >
      Xin Wang</span>, <span >
      Erica Cooper</span>, <span >
      Xiaoxiao Miao</span>, <span >
      Junichi Yamagishi</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2104.01541.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/icassp2022/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/nii-yamagishilab/Attention_Backend_for_ASV" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://openslr.org/82/" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/speaker_verification/">
    Project
  </a>
  







  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=LdZM2yiWHaw" target="_blank" rel="noopener">
  Video
</a>



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICASSP43922.2022.9746688" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/9746688" target="_blank" rel="noopener">
    ICASSP Link</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      <a href="/publication/icassp2022/" >
        <img src="/publication/icassp2022/featured_huae285be974ad89ce1ed7f95b1b287713_430849_150x0_resize_q75_h2_lanczos_3.webp" height="178" width="150"
            alt="Attention Back-end for Automatic Speaker Verification with Multiple Enrollment Utterances" loading="lazy">
      </a>
    
  </div>
</div>

  
    







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/asru2021/" >DeepLip: A Benchmark for Deep Learning-Based Audio-Visual Lip Biometrics</a>
    </div>

    
    <a href="/publication/asru2021/"  class="summary-link">
      <div class="article-style">
        speaker recognition, audio-visual, lip biometrics, deep learning, visual speech.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Meng Liu</span>, <span >
      Longbiao Wang</span>, <span >
      Kong Aik Lee</span>, <span >
      Hanyi Zhang</span>, <span >
      Chang Zeng</span>, <span >
      Jianwu Dang</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/asru2021/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/speaker_verification/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ASRU51503.2021.9688240" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/9688240" target="_blank" rel="noopener">
    ASRU Link</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      <a href="/publication/asru2021/" >
        <img src="/publication/asru2021/featured_hu3cc7554004a4f82bf46186159a47e918_324993_150x0_resize_q75_h2_lanczos.webp" height="84" width="150"
            alt="DeepLip: A Benchmark for Deep Learning-Based Audio-Visual Lip Biometrics" loading="lazy">
      </a>
    
  </div>
</div>

  

  

  

</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2023 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.46271ef31da3f018e9cd1b59300aa265.js"></script>




  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  







<script id="page-data" type="application/json">{"use_headroom":true}</script>



  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.8621d706896ab6d11dd8fd7042e20931.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>


















</body>
</html>
